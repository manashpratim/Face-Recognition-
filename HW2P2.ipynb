{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2P2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNfcKSSxfH1CeP3tvR4P8yC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manashpratim/Face-Recognition-/blob/master/HW2P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItV_79xjh619",
        "colab_type": "code",
        "outputId": "c59eaf1c-5419-4d62-8892-b5be7fc6e7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a145c0899d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87sgqcYlh_Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIYHE1t-iNLJ",
        "colab_type": "code",
        "outputId": "24edeecb-ac98-4c73-ac04-bc242f27866a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms \n",
        "import torchvision \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import sklearn.metrics\n",
        "import sys\n",
        "from torch.utils import data\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "cuda = torch.cuda.is_available()\n",
        "cuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFg3sw_9nOTo",
        "colab_type": "text"
      },
      "source": [
        "# **Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFbkIlXKiNOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/train_data.zip'\n",
        "!unzip '/content/drive/My Drive/validation_classification.zip'\n",
        "!unzip '/content/drive/My Drive/test_classification.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7yNUA2UiNth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, file_list):\n",
        "        self.file_list = file_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.file_list[index])\n",
        "        img = torchvision.transforms.ToTensor()(img)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyS4ZsJYiNVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set= torchvision.datasets.ImageFolder(root='/content/train_data/medium', transform=torchvision.transforms.ToTensor())\n",
        "train_loader= DataLoader(train_set, batch_size=256, shuffle=True, num_workers=8)\n",
        "\n",
        "val_set= torchvision.datasets.ImageFolder(root='/content/validation_classification/medium', transform=torchvision.transforms.ToTensor())\n",
        "val_loader= DataLoader(val_set, batch_size=128, shuffle=False, num_workers=8)\n",
        "\n",
        "test_file = open(\"/content/drive/My Drive/test_order_classification.txt\",\"r\").read().split('\\n')\n",
        "test_files = ['/content/test_classification/medium/'+i for i in test_file ]\n",
        "\n",
        "# Use either of the two lines below. Comment one of them.\n",
        "test_set= torchvision.datasets.ImageFolder(root='/content/test_classification/', transform=torchvision.transforms.ToTensor())\n",
        "test_set = ImageDataset(test_files)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGCfLyce5xPX",
        "colab_type": "text"
      },
      "source": [
        "# **ResNet 50**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_4K-Ny153n0",
        "colab_type": "text"
      },
      "source": [
        "**PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d90xggWqjQv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class convolution(torch.nn.Module):\n",
        "  \n",
        "    def __init__(self,in_channels,out_channels,stride=1):\n",
        "        super(convolution,self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels,out_channels, kernel_size=1,stride=1,bias = False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3,stride=stride, padding=1,bias = False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = torch.nn.Conv2d(out_channels,out_channels*4,kernel_size=1, stride=1, bias = False)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(out_channels*4)\n",
        "\n",
        "        self.conv3_sc = torch.nn.Conv2d(in_channels,out_channels*4,kernel_size=1,stride=stride,bias = False)\n",
        "        self.bn3_sc = torch.nn.BatchNorm2d(out_channels*4)\n",
        "   \n",
        "    def forward(self,x):\n",
        "        res = x\n",
        "        output = F.relu(self.bn1(self.conv1(x)))\n",
        "        output = F.relu(self.bn2(self.conv2(output)))\n",
        "        output = self.bn3(self.conv3(output))\n",
        "\n",
        "        res = self.bn3_sc(self.conv3_sc(res))\n",
        "        output = output + res\n",
        "        output = F.relu(output)\n",
        "        return output\n",
        "\n",
        "class identity(torch.nn.Module):\n",
        "  \n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(identity,self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels,out_channels, kernel_size=1,stride=1,bias = False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3,stride=1, padding=1,bias = False)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = torch.nn.Conv2d(out_channels,out_channels*4,kernel_size=1, stride=1, bias = False)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(out_channels*4)\n",
        "\n",
        "  \n",
        "    def forward(self,x):\n",
        "        res = x\n",
        "        output = F.relu(self.bn1(self.conv1(x)))\n",
        "        output = F.relu(self.bn2(self.conv2(output)))\n",
        "        output = self.bn3(self.conv3(output))\n",
        "\n",
        "        output = output + res\n",
        "        output = F.relu(output)\n",
        "        return output\n",
        "\n",
        "class Resnet50(torch.nn.Module):\n",
        "    def __init__(self,classes=2300):\n",
        "        super(Resnet50,self).__init__()\n",
        "        self.input_planes = 64\n",
        "        self.conv1 = torch.nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias = False)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer11 = convolution(64,64,stride=1)\n",
        "        self.layer12 = identity(256,64)\n",
        "        self.layer13 = identity(256,64)\n",
        "\n",
        "        self.layer21 = convolution(256,128,stride=2)\n",
        "        self.layer22 = identity(512,128)\n",
        "        self.layer23 = identity(512,128)\n",
        "        self.layer24 = identity(512,128)\n",
        "\n",
        "        self.layer31 = convolution(512,256,stride=2)\n",
        "        self.layer32 = identity(1024,256)\n",
        "        self.layer33 = identity(1024,256)\n",
        "        self.layer34 = identity(1024,256)\n",
        "        self.layer35 = identity(1024,256)\n",
        "        self.layer36 = identity(1024,256)\n",
        "\n",
        "        self.layer41 = convolution(1024,512,stride=2)\n",
        "        self.layer42 = identity(2048,512)\n",
        "        self.layer43 = identity(2048,512)\n",
        "\n",
        "        self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
        "        \n",
        "        self.fc = torch.nn.Linear(2048,classes,bias = False)\n",
        "        \n",
        "    def forward(self,x):\n",
        "\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        x = self.layer11(x)\n",
        "        x = self.layer12(x)\n",
        "        x = self.layer13(x)\n",
        "        \n",
        "        x = self.layer21(x)\n",
        "        x = self.layer22(x)\n",
        "        x = self.layer23(x)\n",
        "        x = self.layer24(x)\n",
        "        x = self.layer24(x)\n",
        "\n",
        "        x = self.layer31(x)\n",
        "        x = self.layer32(x)\n",
        "        x = self.layer33(x)\n",
        "        x = self.layer34(x)\n",
        "        x = self.layer35(x)\n",
        "        x = self.layer36(x)\n",
        "\n",
        "        x = self.layer41(x)\n",
        "        x = self.layer42(x)\n",
        "        x = self.layer43(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)/torch.norm(self.fc.weight,dim=1)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXJVUVBuiNco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, data_loader, test_loader,numEpochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        train_loss = []\n",
        "        accuracy = 0\n",
        "        total = 0\n",
        "        start_time = time.time()\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs,_ = model(feats)\n",
        "\n",
        "            _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, labels.long())\n",
        "\n",
        "            accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "            total += len(labels)\n",
        "            train_loss.extend([loss.item()]*feats.size()[0])\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 1000 == 999:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/1000))\n",
        "                avg_loss = 0.0    \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "            del outputs\n",
        "            del pred_labels\n",
        "\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/model2.pt')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            test_loss = []\n",
        "            test_accuracy = 0\n",
        "            test_total = 0\n",
        "\n",
        "            for t_batch_num, (t_feats, t_labels) in enumerate(test_loader):\n",
        "                t_feats, t_labels = t_feats.to(device), t_labels.to(device)\n",
        "                t_outputs,_ = model(t_feats)\n",
        "\n",
        "                _, t_pred_labels = torch.max(F.softmax(t_outputs, dim=1), 1)\n",
        "                t_pred_labels = t_pred_labels.view(-1)\n",
        "\n",
        "                t_loss = criterion(t_outputs, t_labels.long())\n",
        "\n",
        "                test_accuracy += torch.sum(torch.eq(t_pred_labels, t_labels)).item()\n",
        "                test_total += len(t_labels)\n",
        "                test_loss.extend([t_loss.item()]*t_feats.size()[0])\n",
        "                torch.cuda.empty_cache()\n",
        "                del t_feats\n",
        "                del t_labels\n",
        "                del t_loss\n",
        "                del t_pred_labels\n",
        "                del t_outputs\n",
        "        \n",
        "        model.train()\n",
        "        end_time = time.time()\n",
        "            \n",
        "        if task == 'Classification':\n",
        "\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}\\t Time: {:.4f}s'.format(np.mean(train_loss),accuracy/total, np.mean(test_loss),test_accuracy/test_total,end_time - start_time))\n",
        "        else:\n",
        "            test_verify(model, test_loader) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z76LX2ByiNga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI9KQdU1iNnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numEpochs = 12\n",
        "model = Resnet50()\n",
        "model.apply(init_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "#scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1, verbose=True)\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN8ZE6oIlsGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_classify(model,test_loader):\n",
        "  pred = []\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      for batch_num, feats in enumerate(test_loader):\n",
        "              feats = feats.to(device)\n",
        "              \n",
        "              outputs = model(feats)\n",
        "\n",
        "              _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "              pred_labels = pred_labels.view(-1)\n",
        "              pred.append(train_set.classes[pred_labels.cpu().numpy().tolist()[0]])\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_L9ml9kiN3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = test_classify(model,test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzkLApzPiN98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/mbarman_class.csv', 'w') as w:\n",
        "    w.write('Id,Category\\n')\n",
        "    for i in range(len(pred)):\n",
        "            w.write(str(test_file[i])+','+str(pred[i])+'\\n')\n",
        "            if i==0:\n",
        "              print(str(test_file[i])+','+str(pred[i])+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAIq4QmnI3d",
        "colab_type": "text"
      },
      "source": [
        "# **Verification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY_OxxZyiOEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/drive/My Drive/validation_verification.zip'\n",
        "!unzip '/content/drive/My Drive/test_verification.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0xzzmSriOB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ver_dev_file = open(\"/content/drive/My Drive/validation_trials_verification.txt\",\"r\").read().split('\\n')\n",
        "ver_test_file = open(\"/content/drive/My Drive/test_trials_verification.txt\",\"r\").read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHLqrHqWiN6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, file_list1,file_list2):\n",
        "        self.file_list1 = file_list1\n",
        "        self.file_list2 = file_list2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img1 = Image.open(self.file_list1[index])\n",
        "        img1 = torchvision.transforms.ToTensor()(img1)\n",
        "        img2 = Image.open(self.file_list2[index])\n",
        "        img2 = torchvision.transforms.ToTensor()(img2)\n",
        "        return img1,img2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzABCwP7mb98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ver_col1 = []\n",
        "ver_col2 = []\n",
        "ver_labels = []\n",
        "\n",
        "for i in range(len(ver_dev_file)-1):\n",
        "     \n",
        "      p = ver_dev_file[i].split()\n",
        "\n",
        "      ver_col1.append('/content/test_verification/'+p[0]) \n",
        "      ver_col2.append('/content/test_verification/'+p[1]) \n",
        "      ver_labels.append(int(p[2])) \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YfadMkxiN06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ver_set = VerificationDataset(ver_col1,ver_col2)\n",
        "ver_loader = DataLoader(ver_set, batch_size=1024, shuffle=False, num_workers=8, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fueS0r6iNyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "import sklearn\n",
        "\n",
        "dist=[]\n",
        "with torch.no_grad():\n",
        "      \n",
        "      model.eval()\n",
        "      for batch_num, (feats1,feats2) in enumerate(ver_loader):\n",
        "              feats1, feats2 = feats1.to(device), feats2.to(device)\n",
        "              \n",
        "              output1 = model(feats1)\n",
        "              output2 = model(feats2)\n",
        "     \n",
        "              embed_train = np.concatenate((output1.cpu(), output2.cpu()), axis=0)\n",
        "              pca_model = KernelPCA(n_components=100,kernel='cosine',n_jobs=-1)\n",
        "              pca_model.fit(embed_train)\n",
        "              embed1 = pca_model.transform(output1.cpu())\n",
        "              embed2 = pca_model.transform(output2.cpu())\n",
        "             \n",
        "              for i in range(embed1.shape[0]):\n",
        "                  \n",
        "                  dist2 = sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embed1[i], axis=0),np.expand_dims(embed2[i], axis=0))\n",
        "                 \n",
        "                  dist.append(dist2[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-cl5i4tiNq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/mbarman_ver.csv', 'w') as w:\n",
        "    w.write('trial,score\\n')\n",
        "    for i in range(len(dist)):\n",
        "            w.write(str(ver_test_file[i].split()[0])+' '+str(ver_test_file[i].split()[1])+','+str(dist[i])+'\\n')\n",
        "            if i==0:\n",
        "              print(str(ver_test_file[i].split()[0])+' '+str(ver_test_file[i].split()[1])+','+str(dist[i])+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVB6A2sk5JHD",
        "colab_type": "text"
      },
      "source": [
        "#**Keras**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1P9dxIz5JC-",
        "colab_type": "text"
      },
      "source": [
        "**ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtnmVBd5XQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bottleneck(x,f,s):\n",
        "\n",
        "      shortcut = x\n",
        "      out = tf.keras.layers.Conv2D(f,  kernel_size = (1, 1), strides = (1,1), use_bias=False)(x)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "    \n",
        "      out = tf.keras.layers.Conv2D(f,  kernel_size = (3, 3), strides = (s,s), padding = 'same', use_bias=False)(out)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "\n",
        "      out = tf.keras.layers.Conv2D(f*4,  kernel_size = (1, 1), strides = (1,1), use_bias=False)(out)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "     \n",
        "      shortcut = tf.keras.layers.Conv2D(f*4,  kernel_size = (1, 1), strides = (s,s), use_bias=False)(shortcut)\n",
        "      shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    \n",
        "      out = tf.keras.layers.Add()([out, shortcut])\n",
        "      out = tf.keras.activations.relu(out)\n",
        "      \n",
        "      return out\n",
        "\n",
        "def identity(x,f):\n",
        "\n",
        "      shortcut = x\n",
        "      out = tf.keras.layers.Conv2D(f,  kernel_size = (1, 1), strides = (1,1), use_bias=False)(x)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "\n",
        "      out = tf.keras.layers.ZeroPadding2D()(out)\n",
        "      out = tf.keras.layers.Conv2D(f,  kernel_size = (3, 3), strides = (1,1), use_bias=False)(out)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "\n",
        "      out = tf.keras.layers.Conv2D(f*4,  kernel_size = (1, 1), strides = (1,1), use_bias=False)(out)\n",
        "      out = tf.keras.layers.BatchNormalization()(out)\n",
        "      out = tf.keras.activations.relu(out)\n",
        "\n",
        "      out = tf.keras.layers.Add()([out, shortcut])\n",
        "      out = tf.keras.activations.relu(out)\n",
        "      \n",
        "      return out\n",
        "\n",
        "def ResNet50(input_shape = (32, 32, 3), classes = 2300):\n",
        "\n",
        "      input = tf.keras.Input(shape=input_shape)\n",
        "      x = tf.keras.layers.Conv2D(64,  use_bias=False, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(input)\n",
        "      x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.activations.relu(x)\n",
        "\n",
        "      x = bottleneck(x, f = 64, s = 1)\n",
        "      x = identity(x, f = 64)\n",
        "      x = identity(x, f = 64)\n",
        "\n",
        "      x = bottleneck(x, f = 128, s = 2)\n",
        "      x = identity(x, f = 128)\n",
        "      x = identity(x, f = 128)\n",
        "      x = identity(x, f = 128)\n",
        "\n",
        "     \n",
        "      x = bottleneck(x, f = 256, s = 2)\n",
        "      x = identity(x, f = 256)\n",
        "      x = identity(x, f = 256)\n",
        "      x = identity(x, f = 256)\n",
        "      x = identity(x, f = 256)\n",
        "      x = identity(x, f = 256)\n",
        "\n",
        "\n",
        "      x = bottleneck(x, f = 512, s = 2)\n",
        "      x = identity(x, f = 512)\n",
        "      x = identity(x, f = 512)\n",
        "\n",
        "      x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "      x = tf.keras.layers.Dense(classes, use_bias=False, activation='softmax')(x)\n",
        "      model = tf.keras.models.Model(inputs = input, outputs = x, name='ResNet50')\n",
        "      return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AztLD_r15jNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet50()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}